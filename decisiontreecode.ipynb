{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469a74c9",
   "metadata": {},
   "source": [
    "## 1.code to implement decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b87190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creating a decision tree node, which stores the input:-(x), output:-(y), and the index, which is helpful while creating the graph for the decision tree\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x            # feature to split upon 0 indexing.\n",
    "        self.y = y            # output data from node.\n",
    "        self.children = {}    # This hashmap is basically to store the children of a parent node\n",
    "        self.index = -1       # Index, which is helpful while creating graph using pydotplus library,it will be used to assign a unique index to each node.\n",
    "        \n",
    "    # Function/method to add a child to a node\n",
    "    def add_child(self, feature, data):  \n",
    "        self.children[feature] = data   # A new child is added to a node, based on the feature on which it was split upon\n",
    "        \n",
    "        \n",
    "# The main class, with all required functions\n",
    "class DecisionTree:\n",
    "    \n",
    "    # Constructor which initialises a decision tree classfier with a root which initially is None\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "        \n",
    "        \n",
    "    # Returns a dictionary with keys as unique values of output data and the corresponding value as its frequency of that output data\n",
    "    def __numUniqueOp(self, y):\n",
    "        d = {}\n",
    "        for i in y:\n",
    "            if i not in d:\n",
    "                d[i] = 1\n",
    "            else:\n",
    "                d[i] += 1\n",
    "                \n",
    "        return d\n",
    "    \n",
    "\n",
    "    # Function to calculate the gini index of a particular node \n",
    "    def __gini_index(self,y):\n",
    "        possibleOps = self.__numUniqueOp(y)\n",
    "        gini_ind = 1\n",
    "        tot = len(y)\n",
    "        for i in possibleOps:\n",
    "            p = possibleOps[i]/tot\n",
    "            gini_ind -= p**2\n",
    "                                         \n",
    "        return gini_ind\n",
    "    \n",
    "    # Function to calculate the gini gain for a selected feature(f) to split upon\n",
    "    def __gain_gini(self, x, y, f):\n",
    "        \n",
    "        gini_intial = self.__gini_index(y)  # initial gini index before splitting\n",
    "        gini_final = 0\n",
    "        possible_x = set(x[:, f]) # All the unique values for the gini index\n",
    "        df = pd.DataFrame(x)   # Converting the required input into a data frame\n",
    "        df[df.shape[1]] = y    # Appending the output, for the considered input\n",
    "        \n",
    "        initial_size = df.shape[0]\n",
    "        \n",
    "        for i in possible_x:\n",
    "            df1 = df[df[f] == i]   # Selecting the new set of input for the selected feature f and its value i\n",
    "            size = df1.shape[0]\n",
    "            \n",
    "            gini_final += (size/initial_size)*(self.__gini_index(df1[df1.shape[1]-1]))\n",
    "        \n",
    "        gini_gain = gini_intial - gini_final\n",
    "        return gini_gain\n",
    "    \n",
    "    \n",
    "     # Function for calculating the entropy for a given state/node in a decision tree\n",
    "    def __entropy(self, y):\n",
    "        possibleOps = self.__numUniqueOp(y)  # All the unique output data possible for given input data\n",
    "        entro = 0\n",
    "        tot = len(y)\n",
    "        for i in possibleOps:  \n",
    "            pi = possibleOps[i]/tot\n",
    "            entro += (-pi)*(m.log(pi,2))    \n",
    "            \n",
    "        return entro\n",
    "    \n",
    "    # Function to calculate the gain ratio for a given input, output and selected feature\n",
    "    def __gain_ratio(self, x, y, f):\n",
    "        curr_entro = self.__entropy(y)   # Current entropy of the parent node\n",
    "        info_new = 0\n",
    "        split_info = 0\n",
    "        possible_x = set(x[:, f])        # Required input for selected featire \"f\"\n",
    "        df = pd.DataFrame(x)             # Converting the required input into a data frame\n",
    "        df[df.shape[1]] = y              # Appending the output, for the considered input\n",
    "        \n",
    "        initial_size = df.shape[0]      \n",
    "        \n",
    "        for i in possible_x:\n",
    "            df1 = df[df[f] == i]     # Selecting the new set of input for the selected feature f and its value i\n",
    "            size = df1.shape[0]\n",
    "            \n",
    "            info_new += (size/initial_size)*self.__entropy(df1[df1.shape[1]-1])\n",
    "            split_info += (-size/initial_size)*m.log((size/initial_size), 2)\n",
    "            \n",
    "        if split_info == 0:      # To handle 0 denominator error, as Gain ratio = info_gain/split_info\n",
    "            return math.inf\n",
    "        \n",
    "        info_gain = curr_entro - info_new\n",
    "        return info_gain/split_info\n",
    "    \n",
    "    # A helper function, to construct a decision tree\n",
    "    # This function returns the root of decision tree comstructed from the training data\n",
    "    def __build_dt_helper(self, x, y, unused_features, possible_ops, lvl, metric):\n",
    "        # possible_ops are basically the output classes\n",
    "        # metric parameter has a default value of gain_ratio, but can also be changed to gini_index manually.\n",
    "        # depth here is level\n",
    "        \n",
    "        ### Base cases ###\n",
    "        \n",
    "        # If only one output is possible at a node, that means that particular node is pure(leaf node)\n",
    "        if len(set(y)) == 1:   \n",
    "            print(\"Level\",lvl)\n",
    "            op = None\n",
    "            for i in possible_ops:\n",
    "                if i in y:\n",
    "                    op = i\n",
    "                    print(\"Count of\", i,\"=\",len(y))\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Count of\", i,\"=\",0)\n",
    "                    \n",
    "            if metric == \"gain_ratio\":\n",
    "                print(\"Current entropy is 0.0\")     # For a pure node, entropy = 0\n",
    "            else:\n",
    "                print(\"Current Gini index is 0.0\")\n",
    "            print(\"Reached leaf Node\")\n",
    "            print()\n",
    "            return DecisionTreeNode(None,op)    # Returning the node to build the tree\n",
    "        \n",
    "        \n",
    "        # If all the features availble are used ,then we take the majority vote and then classify yhe values in the node(leaf node)\n",
    "        if len(unused_features) == 0:\n",
    "            print(\"Level\",lvl)\n",
    "            possible_vals = self.__numUniqueOp(y)\n",
    "            op = None\n",
    "            maj = -m.inf\n",
    "            for i in possible_ops:\n",
    "                if i not in possible_vals:\n",
    "                    print(\"Count of\", i,\"=\",0)\n",
    "                    \n",
    "                else:\n",
    "                    if possible_vals[i] > maj:\n",
    "                        maj = possible_vals[i]\n",
    "                        op = i\n",
    "                    print(\"Count of\", i,\"=\",possible_vals[i])\n",
    "    \n",
    "            print(\"Reached leaf Node\")\n",
    "            print()\n",
    "            return DecisionTreeNode(None,op)    # Returning the node to build a decision tree\n",
    "        \n",
    "        \n",
    "        # To find maximum gain ratio by splitting the with different features and then selecting the best feature\n",
    "        max_gain = -m.inf\n",
    "        best_feaure = \"\"\n",
    "        for f in unused_features:\n",
    "            if metric == \"gain_ratio\":\n",
    "                curr_gain = self.__gain_ratio(x,y,f)\n",
    "                \n",
    "            else:\n",
    "                curr_gain = self.__gain_gini(x,y,f)\n",
    "            \n",
    "            if curr_gain > max_gain:   # if current gain is better than max gain by selecting a feature \"f\", that feature will be the best feature to split the data upon  \n",
    "                max_gain = curr_gain\n",
    "                best_feaure = f\n",
    "        \n",
    "        print(\"Level\",lvl)\n",
    "        possible_vals = self.__numUniqueOp(y)\n",
    "        op = None\n",
    "        max_count = -m.inf\n",
    "        \n",
    "        for i in possible_ops:\n",
    "            if i not in possible_vals:\n",
    "                print(\"Count of\",i,\"=\",0)\n",
    "                \n",
    "            else:\n",
    "                if possible_vals[i] > max_count:\n",
    "                    op = i\n",
    "                    max_count = possible_vals[i]\n",
    "                    print(\"Count of\",i,\"=\",possible_vals[i])\n",
    "                    \n",
    "    \n",
    "        if metric == \"gain_ratio\":\n",
    "            print(\"Current entropy is\", self.__entropy(y))\n",
    "            print(\"Splitting on the feature \",best_feaure,\" with gain ratio \",max_gain)\n",
    "        else:\n",
    "            print(\"Current Gini index is\",self.__gini_index(y))\n",
    "            print(\"Splitting on the feature \",best_feaure,\" with gain gain \",max_gain)\n",
    "        \n",
    "        print()\n",
    "                                         \n",
    "                                         \n",
    "        # unique_x here represents the unique values of the input for the feature selected\n",
    "        unique_x = set(x[:, best_feaure])\n",
    "        df = pd.DataFrame(x)  \n",
    "        df[df.shape[1]] = y\n",
    "                                        \n",
    "        # Adding a new node by selecting the best feature and passing the output \n",
    "        curr_node = DecisionTreeNode(best_feaure, op)\n",
    "        \n",
    "        # Removing the selected feature from the unsued features \n",
    "        unused_features.remove(best_feaure)\n",
    "        \n",
    "        \n",
    "        for i in unique_x:\n",
    "            df1 = df[df[best_feaure] == i]\n",
    "            x_new = df1.iloc[:,0:df1.shape[1]-1].values  \n",
    "            y_new = df1.iloc[:,df1.shape[1]-1].values\n",
    "            \n",
    "            # Performing a pre-order traversal to continue the process\n",
    "            child_node = self.__build_dt_helper(x_new,y_new, unused_features,possible_ops ,lvl+1, metric)  \n",
    "            \n",
    "            # Adding the child node to the current new node\n",
    "            curr_node.add_child(i, child_node)\n",
    "            \n",
    "        # Returning the current node, basically the head of the decision tree\n",
    "        return curr_node\n",
    "    \n",
    "    \n",
    "    # Fuction to fit the training data, metric here is default set to entropy\n",
    "    def fit(self, x, y, metric = \"entropy\"):\n",
    "        unused_features = [f for f in range(len(x[0]))]\n",
    "        possible_ops = set(y)\n",
    "        \n",
    "        if metric == \"entropy\":\n",
    "            metric = \"gain_ratio\"\n",
    "            \n",
    "        else:\n",
    "            metric = \"gini_index\"\n",
    "        self.__root = self.__build_dt_helper(x, y, unused_features, possible_ops , 0, metric)\n",
    "        \n",
    "        \n",
    "    # Helper function to predict data\n",
    "    def __predict_Helper(self, data, node):\n",
    "        # Base case\n",
    "        if len(node.children) == 0:\n",
    "            return node.y\n",
    "        \n",
    "        # selecting the data on which the feature is split upon\n",
    "        curr_val = data[node.x]\n",
    "           \n",
    "        # Returning the output\n",
    "        if curr_val not in node.children:\n",
    "            return node.y\n",
    "        \n",
    "        # recursively predicting for other given data as well \n",
    "        return self.__predict_Helper(data, node.children[curr_val])\n",
    "        \n",
    "                                         \n",
    "    # Predicts for the given data\n",
    "    def predict(self, x):\n",
    "        y = np.array([0]*len(x))\n",
    "        for i in range(len(x)):\n",
    "            y[i] = self.__predict_Helper(x[i], self.__root)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    # Fucntion to return the score of the classifer\n",
    "    def score(self, x_test, y_true):\n",
    "        y_pred = self.predict(x_test)\n",
    "        count = 0\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == y_test[i]:\n",
    "                count += 1\n",
    "                \n",
    "        return count/len(y_pred)\n",
    "    \n",
    "    # Fuction for creating a visual representation of Decision Tree using pydotplus\n",
    "    def export_tree(self, filename = None):\n",
    "        import pydotplus\n",
    "        import queue\n",
    "        \n",
    "        # Level order traversal on the entire tree and then printing them into a graphiz graph\n",
    "        count = 0\n",
    "        dot_data = '''digraph Tree {node [shape=box] ;'''\n",
    "        q = queue.Queue()\n",
    "    \n",
    "        root = self.__root\n",
    "        q.put(root)\n",
    "        if root.index == -1:\n",
    "            root.index = count\n",
    "            \n",
    "        dot_data = dot_data + \"\\n{} [label=\\\"Feature to split upon : X[{}]\\\\nOutput at this node : {}\\\" ];\".format(count,root.x,root.y)\n",
    "        \n",
    "        while (not q.empty()):\n",
    "            curr_node = q.get()\n",
    "            for i in curr_node.children:\n",
    "                count += 1\n",
    "                if (curr_node.children[i].index == -1):\n",
    "                    curr_node.children[i].index = count\n",
    "                    \n",
    "                # Connecting the children to the node(parent)\n",
    "                dot_data += \"\\n{} [label=\\\"Feature to split upon : X[{}]\\\\nOutput at this node : {}\\\" ];\".format(curr_node.children[i].index,curr_node.children[i].x,curr_node.children[i].y)\n",
    "                \n",
    "                dot_data += \"\\n{} -> {} [ headlabel=\\\"Feature value = {}\\\"]; \".format(curr_node.index,curr_node.children[i].index,i)\n",
    "                \n",
    "                # Adding the children nodes into the queue\n",
    "                q.put(curr_node.children[i])\n",
    "                \n",
    "        dot_data += \"\\n}\"\n",
    "        \n",
    "                                         \n",
    "        if filename != None:    \n",
    "            graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "            graph.write_pdf(filename)    \n",
    "        \n",
    "        # Returing the graph in form of a dotdata format\n",
    "        return dot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cc286db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of 0 = 37\n",
      "Count of 2 = 41\n",
      "Current entropy is 1.5807197138422104\n",
      "Splitting on the feature  3  with gain ratio  0.35918191528212107\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 23\n",
      "Count of 1 = 0\n",
      "Count of 2 = 0\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 4\n",
      "Current entropy is 0.9182958340544896\n",
      "Splitting on the feature  2  with gain ratio  0.40783617806826955\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 1\n",
      "Count of 2 = 0\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 1\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 1\n",
      "Count of 2 = 0\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 1\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 2\n",
      "Count of 2 = 0\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 6\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 8\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 3\n",
      "Count of 2 = 0\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 1\n",
      "Count of 1 = 0\n",
      "Count of 2 = 0\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 5\n",
      "Count of 2 = 0\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 1\n",
      "Count of 2 = 7\n",
      "Current entropy is 0.5435644431995964\n",
      "Splitting on the feature  1  with gain ratio  0.13618441433138823\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 1\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 1\n",
      "Current entropy is 1.0\n",
      "Splitting on the feature  0  with gain ratio  1.0\n",
      "\n",
      "Level 3\n",
      "Count of 0 = 0\n",
      "Count of 1 = 1\n",
      "Count of 2 = 0\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 3\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 1\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 1\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 3\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 2\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 1\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 2\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 3\n",
      "Count of 2 = 0\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 2\n",
      "Count of 2 = 1\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 5\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 1\n",
      "Count of 1 = 0\n",
      "Count of 2 = 0\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 1\n",
      "Count of 2 = 1\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 4\n",
      "Count of 1 = 0\n",
      "Count of 2 = 0\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 2\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 4\n",
      "Count of 1 = 0\n",
      "Count of 2 = 0\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 11\n",
      "Count of 2 = 0\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 5\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 4\n",
      "Count of 2 = 0\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 4\n",
      "Count of 1 = 0\n",
      "Count of 2 = 0\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 0\n",
      "Count of 1 = 0\n",
      "Count of 2 = 2\n",
      "Current entropy is 0.0\n",
      "Reached leaf Node\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the trianing and testing data using a seed\n",
    "iris = datasets.load_iris()\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
    "\n",
    "# Creating an object of the decison tree classifier built above\n",
    "dt = DecisionTree()\n",
    "\n",
    "# Fitting the input data and printing as specified in the instructions\n",
    "dt.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a21ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d4550d6",
   "metadata": {},
   "source": [
    "## 2.representation of iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "892bb7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digraph Tree {node [shape=box] ;\n",
      "0 [label=\"Feature to split upon : X[3]\\nOutput at this node : 2\" ];\n",
      "1 [label=\"Feature to split upon : X[None]\\nOutput at this node : 0\" ];\n",
      "0 -> 1 [ headlabel=\"Feature value = 0.2\"]; \n",
      "2 [label=\"Feature to split upon : X[2]\\nOutput at this node : 1\" ];\n",
      "0 -> 2 [ headlabel=\"Feature value = 1.5\"]; \n",
      "3 [label=\"Feature to split upon : X[None]\\nOutput at this node : 2\" ];\n",
      "0 -> 3 [ headlabel=\"Feature value = 2.1\"]; \n",
      "4 [label=\"Feature to split upon : X[None]\\nOutput at this node : 2\" ];\n",
      "0 -> 4 [ headlabel=\"Feature value = 2.3\"]; \n",
      "5 [label=\"Feature to split upon : X[None]\\nOutput at this node : 1\" ];\n",
      "0 -> 5 [ headlabel=\"Feature value = 1.2\"]; \n",
      "6 [label=\"Feature to split upon : X[None]\\nOutput at this node : 0\" ];\n",
      "0 -> 6 [ headlabel=\"Feature value = 0.6\"]; \n",
      "7 [label=\"Feature to split upon : X[None]\\nOutput at this node : 1\" ];\n",
      "0 -> 7 [ headlabel=\"Feature value = 1.0\"]; \n",
      "8 [label=\"Feature to split upon : X[1]\\nOutput at this node : 2\" ];\n",
      "0 -> 8 [ headlabel=\"Feature value = 1.8\"]; \n",
      "9 [label=\"Feature to split upon : X[None]\\nOutput at this node : 2\" ];\n",
      "0 -> 9 [ headlabel=\"Feature value = 2.5\"]; \n",
      "10 [label=\"Feature to split upon : X[None]\\nOutput at this node : 1\" ];\n",
      "0 -> 10 [ headlabel=\"Feature value = 1.1\"]; \n",
      "11 [label=\"Feature to split upon : X[None]\\nOutput at this node : 1\" ];\n",
      "0 -> 11 [ headlabel=\"Feature value = 1.6\"]; \n",
      "12 [label=\"Feature to split upon : X[None]\\nOutput at this node : 2\" ];\n",
      "0 -> 12 [ headlabel=\"Feature value = 2.0\"]; \n",
      "13 [label=\"Feature to split upon : X[None]\\nOutput at this node : 0\" ];\n",
      "0 -> 13 [ headlabel=\"Feature value = 0.5\"]; \n",
      "14 [label=\"Feature to split upon : X[None]\\nOutput at this node : 1\" ];\n",
      "0 -> 14 [ headlabel=\"Feature value = 1.7\"]; \n",
      "15 [label=\"Feature to split upon : X[None]\\nOutput at this node : 0\" ];\n",
      "0 -> 15 [ headlabel=\"Feature value = 0.1\"]; \n",
      "16 [label=\"Feature to split upon : X[None]\\nOutput at this node : 2\" ];\n",
      "0 -> 16 [ headlabel=\"Feature value = 2.2\"]; \n",
      "17 [label=\"Feature to split upon : X[None]\\nOutput at this node : 0\" ];\n",
      "0 -> 17 [ headlabel=\"Feature value = 0.3\"]; \n",
      "18 [label=\"Feature to split upon : X[None]\\nOutput at this node : 1\" ];\n",
      "0 -> 18 [ headlabel=\"Feature value = 1.3\"]; \n",
      "19 [label=\"Feature to split upon : X[None]\\nOutput at this node : 2\" ];\n",
      "0 -> 19 [ headlabel=\"Feature value = 1.9\"]; \n",
      "20 [label=\"Feature to split upon : X[None]\\nOutput at this node : 1\" ];\n",
      "0 -> 20 [ headlabel=\"Feature value = 1.4\"]; \n",
      "21 [label=\"Feature to split upon : X[None]\\nOutput at this node : 0\" ];\n",
      "0 -> 21 [ headlabel=\"Feature value = 0.4\"]; \n",
      "22 [label=\"Feature to split upon : X[None]\\nOutput at this node : 2\" ];\n",
      "0 -> 22 [ headlabel=\"Feature value = 2.4\"]; \n",
      "23 [label=\"Feature to split upon : X[None]\\nOutput at this node : 1\" ];\n",
      "2 -> 23 [ headlabel=\"Feature value = 4.5\"]; \n",
      "24 [label=\"Feature to split upon : X[None]\\nOutput at this node : 2\" ];\n",
      "2 -> 24 [ headlabel=\"Feature value = 5.1\"]; \n",
      "25 [label=\"Feature to split upon : X[None]\\nOutput at this node : 1\" ];\n",
      "2 -> 25 [ headlabel=\"Feature value = 4.2\"]; \n",
      "26 [label=\"Feature to split upon : X[None]\\nOutput at this node : 2\" ];\n",
      "2 -> 26 [ headlabel=\"Feature value = 5.0\"]; \n",
      "27 [label=\"Feature to split upon : X[None]\\nOutput at this node : 1\" ];\n",
      "2 -> 27 [ headlabel=\"Feature value = 4.9\"]; \n",
      "28 [label=\"Feature to split upon : X[None]\\nOutput at this node : 2\" ];\n",
      "8 -> 28 [ headlabel=\"Feature value = 2.9\"]; \n",
      "29 [label=\"Feature to split upon : X[0]\\nOutput at this node : 1\" ];\n",
      "8 -> 29 [ headlabel=\"Feature value = 3.2\"]; \n",
      "30 [label=\"Feature to split upon : X[None]\\nOutput at this node : 2\" ];\n",
      "8 -> 30 [ headlabel=\"Feature value = 2.5\"]; \n",
      "31 [label=\"Feature to split upon : X[None]\\nOutput at this node : 2\" ];\n",
      "8 -> 31 [ headlabel=\"Feature value = 3.0\"]; \n",
      "32 [label=\"Feature to split upon : X[None]\\nOutput at this node : 2\" ];\n",
      "8 -> 32 [ headlabel=\"Feature value = 2.7\"]; \n",
      "33 [label=\"Feature to split upon : X[None]\\nOutput at this node : 1\" ];\n",
      "29 -> 33 [ headlabel=\"Feature value = 5.9\"]; \n",
      "34 [label=\"Feature to split upon : X[None]\\nOutput at this node : 2\" ];\n",
      "29 -> 34 [ headlabel=\"Feature value = 7.2\"]; \n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## create a graphical representation from the generated result in form of an object.\n",
    "print(dt.export_tree(filename = \"irisrep.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a374495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8381b4",
   "metadata": {},
   "source": [
    "## create a decision tree for or gate. (output is named as orgate.pdf) will be in documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d20cf6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data=np.array([[1,1,1],\n",
    "             [0,0,0],\n",
    "             [1,0,1],\n",
    "             [0,1,1]])\n",
    "x=data[:,:-1]\n",
    "y=data[:,-1].astype(str)\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=1)\n",
    "clf=DecisionTreeClassifier(criterion=\"entropy\")\n",
    "clf.fit(x_train,y_train)\n",
    "a=np.array(['0','1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "099e61d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "d=export_graphviz(clf,out_file=None,rounded=True,filled=True,class_names=a)\n",
    "g=pydotplus.graph_from_dot_data(d)\n",
    "g.write_pdf(\"orgate.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b4059",
   "metadata": {},
   "source": [
    "## create a decision tree using pydotplus and graphviz for iris dataset \n",
    "## output will be in irisset.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be2a4bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the trianing and testing data using a seed\n",
    "iris = datasets.load_iris()\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
    "clf=DecisionTreeClassifier(criterion=\"entropy\")\n",
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9477fc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "d=export_graphviz(clf,out_file=None,rounded=True,filled=True,feature_names=iris.feature_names,class_names=iris.target_names)\n",
    "g=pydotplus.graph_from_dot_data(d)\n",
    "g.write_pdf(\"orgate.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eba50144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.str_"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iris.target_names[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
